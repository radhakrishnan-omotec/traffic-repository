{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7166601,
          "sourceType": "datasetVersion",
          "datasetId": 4107330
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 68.691048,
      "end_time": "2023-12-12T12:45:53.82776",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-12-12T12:44:45.136712",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/traffic-repository/blob/main/ambulance_traffic_density_estimation_yolov8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"data:image/jpeg;base64,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\" width=\"2400\">"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017921,
          "end_time": "2023-12-12T12:44:48.731519",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.713598",
          "status": "completed"
        },
        "tags": [],
        "id": "EM75mNqQNOkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 Object Detection Overview</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"><strong>Object detection</strong> involves identifying the location and class of objects in images or video streams. Its output includes bounding boxes enclosing each object, along with class labels and confidence scores. This method is ideal for pinpointing objects in a scene when exact shape details are unnecessary. Among the leading models for this task are <strong>RCNNs</strong>, <strong>SSDs</strong>, and <strong>YOLO</strong>. Currently, YOLO models excel, outperforming others in balancing speed and accuracy, particularly in real-time detection scenarios.</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017351,
          "end_time": "2023-12-12T12:44:48.766412",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.749061",
          "status": "completed"
        },
        "tags": [],
        "id": "hTHvJestNOkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚀 YOLO: The Real-Time Object Detection Revolution</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <strong>YOLO</strong>, short for \"<strong>You Only Look Once</strong>\", revolutionizes object detection in computer vision by offering a fast and streamlined approach. It uniquely simplifies the detection process, treating it as a single-step task that combines object localization and classification. This innovative method allows YOLO to process images and videos in real-time with high accuracy. The latest version, <strong>YOLOv8</strong>, further enhances this technology, adeptly tackling challenges in object detection and image segmentation. YOLO's real-time capabilities make it a standout choice for applications needing rapid and precise object identification.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017924,
          "end_time": "2023-12-12T12:44:48.801799",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.783875",
          "status": "completed"
        },
        "tags": [],
        "id": "OzL_XE7WNOkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🌟 YOLOv8: Advancing the Frontiers of Object Detection</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Launched in <strong>January 2023</strong> by <strong>Ultralytics</strong>, <strong>YOLOv8</strong> stands as the latest advancement in the YOLO AI model series. Designed for tasks like classification, object detection, and image segmentation, YOLOv8 outshines its predecessor, YOLOv7, in both precision and speed. Utilizing Darknet53 as its backbone, it employs more feature maps and efficient convolutional neural networks, resulting in higher mAP and fps. YOLOv8 introduces an innovative anchor-free detection head, enabling pixel-wise bounding box estimations akin to image segmentation techniques, and incorporates a new loss function. This culmination of features achieves a <strong>mean Average Precision</strong> of <strong>53.7%</strong> on the <strong>COCO benchmark dataset</strong>, marking YOLOv8 as a state-of-the-art model in object detection.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017046,
          "end_time": "2023-12-12T12:44:48.835958",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.818912",
          "status": "completed"
        },
        "tags": [],
        "id": "AUPGzF9zNOkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚦 Traffic Density Estimation Project Overview</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Leveraging YOLO's real-time detection capabilities, this project focuses on <strong>Traffic Density Estimation</strong>, a vital component in urban and traffic management. The goal is to count vehicles within a specific area in each frame to assess traffic density. This valuable data aids in identifying peak traffic periods, congested zones, and assists in urban planning. Through this project, we aim to develop a comprehensive toolset that provides detailed insights into traffic flow and patterns, enhancing traffic management and city planning strategies.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017002,
          "end_time": "2023-12-12T12:44:48.870172",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.85317",
          "status": "completed"
        },
        "tags": [],
        "id": "EXgZOQJFNOkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🎯 Project Objectives</b></h1>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><strong>YOLOv8 Model Selection and Initial Assessment:</strong> Starting with YOLOv8's pre-trained model selection, assessing its initial performance on COCO dataset for vehicle detection.</li>\n",
        "        <li><strong>Specialized Vehicle Dataset Preparation:</strong> Curating and annotating a vehicle-specific dataset to refine the model's detection capabilities for diverse vehicle types.</li>\n",
        "        <li><strong>Model Fine-Tuning for Enhanced Vehicle Detection:</strong> Employing transfer learning to fine-tune the YOLOv8 model, focusing on vehicle detection from aerial perspectives for improved precision and recall.</li>\n",
        "        <li><strong>Comprehensive Model Performance Evaluation:</strong> Analyzing learning curves, evaluating confusion matrix, and assessing performance metrics to validate the model's accuracy and generalization capabilities.</li>\n",
        "        <li><strong>Inference and Generalization on Test Data:</strong> Testing the model's generalization on validation images, an unseen test image, and a test video to demonstrate its practical application and effectiveness.</li>\n",
        "        <li><strong>Real-Time Traffic Density Estimation:</strong> Implementing an algorithm to estimate traffic density by counting vehicles and analyzing traffic intensity in real-time on test video data.</li>\n",
        "        <li><strong>Cross-Platform Model Deployment Preparation:</strong> Exporting the fine-tuned model in ONNX format for versatile use across different platforms and environments.</li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016939,
          "end_time": "2023-12-12T12:44:48.904292",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.887353",
          "status": "completed"
        },
        "tags": [],
        "id": "3qMPZqUKNOkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"contents_tabel\"></a>   \n",
        "\n",
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📋 Table of Contents</b></h1>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><a href=\"#Initialization\" style=\"text-decor\n",
        "            ation: none;\">Step 1 | Setup and Initialization</a></li>\n",
        "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">Step 2 | Loading YOLOv8 Pre-trained Model</a></li>\n",
        "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">Step 3 | Dataset Exploration</a></li>\n",
        "        <li><a href=\"#Fine_Tuning_YOLOv8\" style=\"text-decoration: none;\">Step 4 | Fine-Tuning YOLOv8</a></li>\n",
        "        <li><a href=\"#Model_Performance_Evaluation\" style=\"text-decoration: none;\">Step 5 | Model Performance Evaluation</a>\n",
        "            <ul>\n",
        "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">Step 5.1 | Learning Curves Analysis</a></li>\n",
        "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">Step 5.2 | Confusion Matrix Evaluation</a></li>\n",
        "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">Step 5.3 | Performance Metrics Assessment</a></li>\n",
        "            </ul>\n",
        "        </li>\n",
        "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">Step 6 | Model Inference & Generalization Assessment</a>\n",
        "            <ul>\n",
        "                <li><a href=\"#Inference_Validation\" style=\"text-decoration: none;\">Step 6.1 | Inference on Validation Set Images</a></li>\n",
        "                <li><a href=\"#Inference_Test_Image\" style=\"text-decoration: none;\">Step 6.2 | Inference on an Unseen Test Image</a></li>\n",
        "                <li><a href=\"#Inference_Test_Video\" style=\"text-decoration: none;\">Step 6.3 | Inference on an Unseen Test Video</a></li>\n",
        "            </ul>\n",
        "        </li>\n",
        "        <li><a href=\"#Traffic_Intensity_Estimation\" style=\"text-decoration: none;\">Step 7 | Real-Time Traffic Intensity Estimation</a></li>\n",
        "        <li><a href=\"#Model_Export\" style=\"text-decoration: none;\">Step 8 | Model Export for Cross-Platform Deployment</a></li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016739,
          "end_time": "2023-12-12T12:44:48.938242",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.921503",
          "status": "completed"
        },
        "tags": [],
        "id": "dBzysc2ONOkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"left\"><font color=#141140>Let's get started:</font></h2>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017755,
          "end_time": "2023-12-12T12:44:48.973007",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.955252",
          "status": "completed"
        },
        "tags": [],
        "id": "9VTFNEMdNOkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Initialization\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 1 | Setup and Initialization</p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016921,
          "end_time": "2023-12-12T12:44:49.006952",
          "exception": false,
          "start_time": "2023-12-12T12:44:48.990031",
          "status": "completed"
        },
        "tags": [],
        "id": "ogvif0VJNOkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">YOLOv8 is the first version of the YOLO series to offer an official package that can be easily installed using pip. This streamlines the setup process, a notable advancement from previous versions that required cloning repositories and running scripts. Let's begin by installing the Ultralytics package:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017387,
          "end_time": "2023-12-12T12:44:49.08451",
          "exception": false,
          "start_time": "2023-12-12T12:44:49.067123",
          "status": "completed"
        },
        "tags": [],
        "id": "87D_TG4xNOkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ultralytics library\n",
        "!pip install ultralytics --upgrade"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": 13.887399,
          "end_time": "2023-12-12T12:45:02.989658",
          "exception": false,
          "start_time": "2023-12-12T12:44:49.102259",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-21T05:06:45.897662Z",
          "iopub.execute_input": "2024-05-21T05:06:45.898053Z",
          "iopub.status.idle": "2024-05-21T05:09:15.872627Z",
          "shell.execute_reply.started": "2024-05-21T05:06:45.898021Z",
          "shell.execute_reply": "2024-05-21T05:09:15.871245Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3I7glbsNOkO",
        "outputId": "540026e3-1c76-4f9f-a66b-4447ee47565b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.18-py3-none-any.whl (757 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Then, let's import all the essential libraries needed for our project:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018229,
          "end_time": "2023-12-12T12:45:03.027025",
          "exception": false,
          "start_time": "2023-12-12T12:45:03.008796",
          "status": "completed"
        },
        "tags": [],
        "id": "HIV1TmekNOkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable warnings in the notebook to maintain clean output cells\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import yaml\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Video"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.616125,
          "end_time": "2023-12-12T12:45:08.66149",
          "exception": false,
          "start_time": "2023-12-12T12:45:03.045365",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2Rde9lX5NOkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the visual appearance of Seaborn plots\n",
        "sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.027946,
          "end_time": "2023-12-12T12:45:08.709398",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.681452",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "83DzvEG2NOkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Load_Model\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 2 | Loading YOLOv8 Pre-trained Model</p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019009,
          "end_time": "2023-12-12T12:45:08.747327",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.728318",
          "status": "completed"
        },
        "tags": [],
        "id": "FPVne7ujNOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Here are the pre-trained YOLOv8 object detection models, which have been trained on the <strong>COCO dataset</strong>. The Common Objects in Context (COCO) dataset is extensive, designed for object detection, segmentation, and captioning, and encompasses <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80 diverse object categories</a>:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018944,
          "end_time": "2023-12-12T12:45:08.785891",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.766947",
          "status": "completed"
        },
        "tags": [],
        "id": "D7SyQ7-jNOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/YOLOv8_object_detection_models.jpg?raw=true\" width=\"2400\">"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018452,
          "end_time": "2023-12-12T12:45:08.823299",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.804847",
          "status": "completed"
        },
        "tags": [],
        "id": "OMfzV_sFNOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📈 Model Performance Trade-offs</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The YOLOv8 suite presents five distinct models: <strong>nano</strong>, <strong>small</strong>, <strong>medium</strong>, <strong>large</strong>, and <strong>xlarge</strong>. A clear trend emerges from the data: as model size increases, there's a notable improvement in <strong>mAP</strong>, indicating enhanced accuracy. Conversely, this augmentation comes at the cost of speed, with larger models being slower. All models adhere to a standard input size of <strong>640x640</strong> pixels, optimizing performance across diverse applications.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018195,
          "end_time": "2023-12-12T12:45:08.860376",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.842181",
          "status": "completed"
        },
        "tags": [],
        "id": "z3cwdkGqNOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Intersection Over Union (IoU)</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        IoU is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between the predicted bounding box and the ground truth, with values ranging from 0 (no overlap) to 1 (perfect overlap). IoU is crucial for determining whether a detection is a true positive or a false positive, often using a threshold like 0.5 or 0.75 to make this distinction.\n",
        "    </p>\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Mean Average Precision (mAP)</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        mAP is a commonly used metric to evaluate the precision of object detection models. It is the average of the AP (Average Precision) calculated for all the classes and is based on the area under the precision-recall curve. This metric reflects the model's precision across different levels of recall, providing a comprehensive performance measure that accounts for both the detection accuracy and the ability to detect all relevant objects.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018694,
          "end_time": "2023-12-12T12:45:08.897562",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.878868",
          "status": "completed"
        },
        "tags": [],
        "id": "uU3h660QNOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">For our real-time traffic density estimation application, I am going to select the <strong>YOLOv8 nano pre-trained model (yolov8n.pt)</strong> to handle vehicle detection. This model ensures the fastest possible inference time, making it well-suited for real-time use:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018603,
          "end_time": "2023-12-12T12:45:08.935075",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.916472",
          "status": "completed"
        },
        "tags": [],
        "id": "WtjPABWcNOkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained YOLOv8n model from Ultralytics\n",
        "model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.536317,
          "end_time": "2023-12-12T12:45:09.490231",
          "exception": false,
          "start_time": "2023-12-12T12:45:08.953914",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "AZR39xNFNOkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">The pre-trained model we've loaded is trained on the COCO dataset, which includes the 'car' and 'truck' classes among its 80 different categories — exactly what we need for our project. Now, let's put our model to the test and see how it performs on a sample image:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018601,
          "end_time": "2023-12-12T12:45:09.528765",
          "exception": false,
          "start_time": "2023-12-12T12:45:09.510164",
          "status": "completed"
        },
        "tags": [],
        "id": "ch2C6kHRNOkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the image file\n",
        "image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
        "\n",
        "# Perform inference on the provided image(s)\n",
        "results = model.predict(source=image_path,\n",
        "                        imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n",
        "                        conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n",
        "\n",
        "# Annotate and convert image to numpy array\n",
        "sample_image = results[0].plot(line_width=2)\n",
        "\n",
        "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
        "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display annotated image\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(sample_image)\n",
        "plt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=20)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 8.967725,
          "end_time": "2023-12-12T12:45:18.515569",
          "exception": false,
          "start_time": "2023-12-12T12:45:09.547844",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "bdVNrJU8NOkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Pre-trained Model Detection Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        In our sample image, the pre-trained model missed the detectable truck and car that were clearly visible. A model pre-trained on a dataset with a broad range of classes, like COCO's 80 different categories, may not perform as well on a specific subset of those categories due to the diversity of objects it has been trained to recognize. If we fine-tune this model on a specialized dataset that focuses solely on vehicles, it can learn to detect various types of vehicles more accurately. Fine-tuning on a vehicle-specific dataset allows the model to become more specialized, adjusting the weights to be more sensitive to features specific to vehicles. As a result, the model's mean Average Precision (mAP) for vehicle detection could improve because it's being optimized on a narrower, more relevant range of classes for our specific application. Fine-tuning also helps the model generalize better for vehicle detection tasks, potentially reducing false negatives (like missing a detectable truck) and improving overall detection performance.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.04642,
          "end_time": "2023-12-12T12:45:18.611612",
          "exception": false,
          "start_time": "2023-12-12T12:45:18.565192",
          "status": "completed"
        },
        "tags": [],
        "id": "XFOmCg6wNOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Dataset_Exploration\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Dataset Exploration</p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046519,
          "end_time": "2023-12-12T12:45:18.70644",
          "exception": false,
          "start_time": "2023-12-12T12:45:18.659921",
          "status": "completed"
        },
        "tags": [],
        "id": "EW0Mh0ERNOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Dataset Preparation for Model Fine-tuning</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To fine-tune our pre-trained model on a specialized dataset that focuses solely on vehicles, so that it can learn to detect various types of vehicles more accurately, I have prepared a dataset which is available at this link <a href=\"https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset\">Top-View Vehicle Detection Image Dataset for YOLOv8</a>. The dataset zeroes in on the 'Vehicle' class, covering a wide variety of vehicles such as cars, trucks, and buses. It is composed of 626 images sourced from top-view perspectives, <strong>annotated meticulously in the YOLOv8 format</strong> for effective vehicle detection.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The dataset undergoes a standardization process where each image is resized to a uniform resolution of <strong>640x640 pixels</strong>. To bolster the model's ability to generalize, augmentations were applied to the training data, which consists of 536 images. The validation set contains 90 images and remains unaugmented to preserve the integrity of performance evaluation.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        This dataset, curated from <a href=\"https://www.pexels.com/search/videos/\">Pexels</a>, captures the diversity of vehicles from an aerial view, making it ideal for highway monitoring tasks. Each video frame was selected at a sampling rate of 1 frame per second using <a href=\"https://universe.roboflow.com/farzad/vehicle_detection_yolov8\">Roboflow</a>, which facilitated precise annotation for object detection.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.04706,
          "end_time": "2023-12-12T12:45:18.800921",
          "exception": false,
          "start_time": "2023-12-12T12:45:18.753861",
          "status": "completed"
        },
        "tags": [],
        "id": "BrlDZEZBNOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ YOLOv8 Dataset Format</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Our dataset, structured for YOLOv8 format, has been meticulously prepared on Roboflow. It encompasses all necessary components for an efficient object detection model training. Here’s a detailed breakdown:\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>1️⃣ Train Directory:</b><br>\n",
        "        The 'train' directory houses our training dataset. It includes 536 images within the 'images' subfolder and corresponding YOLOv8 format labels in the 'labels' subfolder.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>2️⃣ Validation Directory:</b><br>\n",
        "        The 'valid' directory contains the validation dataset. This consists of 90 images in the 'images' subfolder and their respective YOLOv8 format labels in the 'labels' subfolder.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>3️⃣ data.yaml:</b><br>\n",
        "        This file is the Ultralytics YOLO dataset configuration file. It specifies paths to the training and validation datasets, defines the number of classes (1), and the class name ('Vehicle'). This format is crucial for setting up and training the model accurately with our dataset.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>📝 Note about labels:</b><br>\n",
        "        Labels in our dataset are formatted in YOLO style, where each image is associated with a *.txt file. These files describe the detected objects in a '<strong>class x_center y_center width height</strong>' format. Importantly, the box coordinates are normalized between 0 and 1. If an image has no detectable objects, it won’t have a corresponding *.txt file.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046033,
          "end_time": "2023-12-12T12:45:18.893777",
          "exception": false,
          "start_time": "2023-12-12T12:45:18.847744",
          "status": "completed"
        },
        "tags": [],
        "id": "20vCaQ2INOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's begin our exploration by examining the '<strong>data.yaml</strong>' file:</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046651,
          "end_time": "2023-12-12T12:45:18.986753",
          "exception": false,
          "start_time": "2023-12-12T12:45:18.940102",
          "status": "completed"
        },
        "tags": [],
        "id": "RhT8QtN0NOkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset_path\n",
        "dataset_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset'\n",
        "\n",
        "# Set the path to the YAML file\n",
        "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
        "\n",
        "# Load and print the contents of the YAML file\n",
        "with open(yaml_file_path, 'r') as file:\n",
        "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
        "    print(yaml.dump(yaml_content, default_flow_style=False))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.066253,
          "end_time": "2023-12-12T12:45:19.099731",
          "exception": false,
          "start_time": "2023-12-12T12:45:19.033478",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "cbJyfvi_NOkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ Understanding the data.yaml File</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        As I previously mentioned, the 'data.yaml' file is a key part of setting up our model. It points out where to find the training and validation images and tells the model that we're focusing on just one class, named 'Vehicle'. This file is essential for making sure our Ultralytics YOLOv8 model learns from our specific dataset, as it guides the model to understand exactly what it needs to look for and where.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046159,
          "end_time": "2023-12-12T12:45:19.194321",
          "exception": false,
          "start_time": "2023-12-12T12:45:19.148162",
          "status": "completed"
        },
        "tags": [],
        "id": "7tiGq6dnNOkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Now, let's continue our exploration by counting the images in both the training and validation sets and verifying their sizes:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046625,
          "end_time": "2023-12-12T12:45:19.287516",
          "exception": false,
          "start_time": "2023-12-12T12:45:19.240891",
          "status": "completed"
        },
        "tags": [],
        "id": "hitzpVgcNOkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths for training and validation image sets\n",
        "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
        "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
        "\n",
        "# Initialize counters for the number of images\n",
        "num_train_images = 0\n",
        "num_valid_images = 0\n",
        "\n",
        "# Initialize sets to hold the unique sizes of images\n",
        "train_image_sizes = set()\n",
        "valid_image_sizes = set()\n",
        "\n",
        "# Check train images sizes and count\n",
        "for filename in os.listdir(train_images_path):\n",
        "    if filename.endswith('.jpg'):\n",
        "        num_train_images += 1\n",
        "        image_path = os.path.join(train_images_path, filename)\n",
        "        with Image.open(image_path) as img:\n",
        "            train_image_sizes.add(img.size)\n",
        "\n",
        "# Check validation images sizes and count\n",
        "for filename in os.listdir(valid_images_path):\n",
        "    if filename.endswith('.jpg'):\n",
        "        num_valid_images += 1\n",
        "        image_path = os.path.join(valid_images_path, filename)\n",
        "        with Image.open(image_path) as img:\n",
        "            valid_image_sizes.add(img.size)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of training images: {num_train_images}\")\n",
        "print(f\"Number of validation images: {num_valid_images}\")\n",
        "\n",
        "# Check if all images in training set have the same size\n",
        "if len(train_image_sizes) == 1:\n",
        "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
        "else:\n",
        "    print(\"Training images have varying sizes.\")\n",
        "\n",
        "# Check if all images in validation set have the same size\n",
        "if len(valid_image_sizes) == 1:\n",
        "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
        "else:\n",
        "    print(\"Validation images have varying sizes.\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.275063,
          "end_time": "2023-12-12T12:45:23.611095",
          "exception": false,
          "start_time": "2023-12-12T12:45:19.336032",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "x1CGx74oNOkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📊 Dataset Analysis Insights</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The dataset for our project consists of 536 training images and 90 validation images, all uniformly sized at 640x640 pixels. This size aligns with the benchmark standard for the YOLOv8 model, ensuring optimal accuracy and speed during model performance. The split ratio of approximately 85% for training and 15% for validation provides a substantial amount of data for model learning while retaining enough images for effective model validation.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.051006,
          "end_time": "2023-12-12T12:45:23.712196",
          "exception": false,
          "start_time": "2023-12-12T12:45:23.66119",
          "status": "completed"
        },
        "tags": [],
        "id": "0KgvTT8ANOkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's take a look at a few images from the training dataset to get a sense of what the data looks like:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.047477,
          "end_time": "2023-12-12T12:45:23.809043",
          "exception": false,
          "start_time": "2023-12-12T12:45:23.761566",
          "status": "completed"
        },
        "tags": [],
        "id": "5mx5fEdiNOkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all jpg images in the directory\n",
        "image_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n",
        "\n",
        "# Select 8 images at equal intervals\n",
        "num_images = len(image_files)\n",
        "selected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n",
        "\n",
        "# Create a 2x4 subplot\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 11))\n",
        "\n",
        "# Display each of the selected images\n",
        "for ax, img_file in zip(axes.ravel(), selected_images):\n",
        "    img_path = os.path.join(train_images_path, img_file)\n",
        "    image = Image.open(img_path)\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from Training Dataset', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.758546,
          "end_time": "2023-12-12T12:45:26.614041",
          "exception": false,
          "start_time": "2023-12-12T12:45:23.855495",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "7RCPi0_rNOkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Fine_Tuning_YOLOv8\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Fine-Tuning YOLOv8 </p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.095762,
          "end_time": "2023-12-12T12:45:26.808073",
          "exception": false,
          "start_time": "2023-12-12T12:45:26.712311",
          "status": "completed"
        },
        "tags": [],
        "id": "TkYVmh2kNOkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step of our project, we're set to fine-tune our YOLOv8 pre-trained object detection model using transfer learning, specifically tailoring it to our 'Top-View Vehicle Detection Image Dataset'. By leveraging the YOLOv8 model's existing weights from its training on the comprehensive COCO dataset, we start from a robust foundation rather than from scratch. This approach not only saves significant time and resources but also capitalizes on our focused dataset to enhance the model's ability to accurately recognize and detect vehicles in top-view images. This method of training enables efficient and effective model adaptation, ensuring it's finely attuned to the specificities of vehicle detection from aerial perspectives:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.0987,
          "end_time": "2023-12-12T12:45:27.003546",
          "exception": false,
          "start_time": "2023-12-12T12:45:26.904846",
          "status": "completed"
        },
        "tags": [],
        "id": "ubb12X4HNOkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on our custom dataset\n",
        "results = model.train(\n",
        "    data=yaml_file_path,     # Path to the dataset configuration file\n",
        "    epochs=100,              # Number of epochs to train for\n",
        "    imgsz=640,               # Size of input images as integer\n",
        "    device=0,                # Device to run on, i.e. cuda device=0\n",
        "    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n",
        "    batch=32,                # Number of images per batch\n",
        "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
        "    lr0=0.0001,              # Initial learning rate\n",
        "    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n",
        "    dropout=0.1,             # Use dropout regularization\n",
        "    seed=0                   # Random seed for reproducibility\n",
        ")"
      ],
      "metadata": {
        "papermill": {
          "duration": 23.627724,
          "end_time": "2023-12-12T12:45:50.728563",
          "exception": true,
          "start_time": "2023-12-12T12:45:27.100839",
          "status": "failed"
        },
        "tags": [],
        "trusted": true,
        "id": "4yWFaojeNOkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🌐 Integration of Weights & Biases (Wandb) with YOLOv8</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Weights & Biases, known as <strong>wandb.ai</strong>, is an MLOps tool that works seamlessly with Ultralytics, including the YOLOv8 model. When we train our YOLOv8 model, <strong>wandb.ai</strong> helps to manage our machine learning experiments by monitoring the training process, logging important metrics, and saving outputs. It's like a dashboard where we can see how our model is learning, with all the details and visualizations to help us understand the training progress and results.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>🔑 Providing the API Key</b><br>\n",
        "        During the model training, an API key is required for Weights & Biases to track and store our model's data. We'll be prompted to sign up at <a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize</a> to get this key. After signing up, we copy the API key provided and paste it back into our training setup. This key connects our training session to the Weights & Biases platform, allowing us to access all the great features for monitoring and evaluating our model.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "-DjcLuPNNOkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Understanding Run Summary Metrics</b></h2>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Learning Rate per Group (lr/pg0, lr/pg1, lr/pg2):</b> These values represent the learning rate for different groups of layers in the neural network. A lower learning rate means the model updates its weights more slowly during training. Consistent learning rates across groups indicate uniform adjustments during the learning process.</li>\n",
        "        <li><b>Mean Average Precision at 50% IoU (metrics/mAP50(B)):</b> This metric measures the model's accuracy in detecting objects with at least 50% Intersection over Union (IoU) with ground truth. A score of 0.97 suggests the model is highly accurate at this IoU threshold.</li>\n",
        "        <li><b>Mean Average Precision across IoU from 50% to 95% (metrics/mAP50-95(B)):</b> This is an average of mAP calculated at different IoU thresholds, from 50% to 95%. A score of 0.74 indicates good overall accuracy across these varying thresholds.</li>\n",
        "        <li><b>Precision (metrics/precision(B)):</b> Precision measures the ratio of correctly predicted positive observations to the total predicted positives. A score of 0.92 means the model is highly precise in its predictions.</li>\n",
        "        <li><b>Recall (metrics/recall(B)):</b> Recall calculates the ratio of correctly predicted positive observations to all observations in actual class. A recall of 0.94 shows the model is very good at finding all relevant cases within the dataset.</li>\n",
        "        <li><b>Model Computational Complexity (model/GFLOPs):</b> Indicates the model's computational demands, with the GFLOPs value suggesting moderate complexity.</li>\n",
        "        <li><b>Model Parameters:</b> This is the total number of trainable parameters in the model. Almost 3 million parameters indicate a model of moderate size and complexity.</li>\n",
        "        <li><b>Inference Speed (model/speed_PyTorch(ms)):</b> The time taken for the model to make a single prediction (inference). 4.6 ms is quite fast, which is good for real-time applications.</li>\n",
        "        <li><b>Training Losses (train/box_loss, train/cls_loss, train/dfl_loss):</b>These are different types of losses during training. 'box_loss' refers to the error in bounding box predictions, 'cls_loss' to classification error, and 'dfl_loss' to distribution focal loss. Lower values indicate better performance.</li>\n",
        "        <li><b>Validation Losses (val/box_loss, val/cls_loss, val/dfl_loss):</b> Similar to training losses, these are losses calculated on the validation dataset. They give an idea of how well the model generalizes to new, unseen data. Almost similar loss values for both training and validation indicate that the model is not overfitting.</li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "l4MLXa7lNOkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Model_Performance_Evaluation\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Model Performance Evaluation </p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "NPYprmftNOkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Post-training, our model generates several files and folders that encapsulate various aspects of the training run. Let's see the list of generated files:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "GrM5wQU-NOkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the directory\n",
        "post_training_files_path = '/kaggle/working/runs/detect/train'\n",
        "\n",
        "# List the files in the directory\n",
        "!ls {post_training_files_path}"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "GA8tK4BhNOkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📁 Training Output Files Explained</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Here’s a rundown of each item:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Weights Folder:</b> Contains the 'best.pt' and 'last.pt' files, which are the best and most recent weights of our trained model respectively.</li>\n",
        "        <li><b>Args:</b> A file that stores the arguments or parameters that were used during the training process.</li>\n",
        "        <li><b>Confusion Matrix:</b> Visual representations of the model performance. One is normalized, which helps in understanding the true positive rate across classes.</li>\n",
        "        <li><b>Events File:</b> Contains logs of events that occurred during training, useful for debugging and analysis.</li>\n",
        "        <li><b>F1 Curve:</b> Illustrates the F1 score of the model over time, balancing precision and recall.</li>\n",
        "        <li><b>Labels:</b> Shows the distribution of different classes within the dataset and their correlation.</li>\n",
        "        <li><b>P Curve, PR Curve, R Curve:</b> These are Precision, Precision-Recall, and Recall curves, respectively, providing insights into the trade-offs between different metrics at various thresholds.</li>\n",
        "        <li><b>results:</b> This csv file captures a comprehensive set of performance metrics recorded at each epoch during the model's training process.</li>\n",
        "        <li><b>Train Batch Images:</b> Sample images from the training set with model predictions overlaid, useful for a quick visual check of model performance.</li>\n",
        "        <li><b>Validation Batch Images:</b> Similar to train batch images, these are from the validation set and include both labels and predictions, providing a glimpse into how well the model generalizes.</li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "l_CiJpw0NOkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        I will undertake a comprehensive evaluation and analysis of our model's performance, which involves:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Learning Curves Analysis</b></li>\n",
        "        <li><b>Confusion Matrix Evaluation</b></li>\n",
        "        <li><b>Performance Metrics Assessment</b></li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "UzYJ1mNkNOkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Learning_Curves\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 5.1 |</span><span style='color:#141140'> Learning Curves Analysis</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "MlNuduz9NOkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step, let's review the training and validation loss trends over epochs to assess the learning stability and efficiency:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "pfc3i-n2NOkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot learning curves for loss values\n",
        "def plot_learning_curve(df, train_loss_col, val_loss_col, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='#141140', linestyle='-', linewidth=2)\n",
        "    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='orangered', linestyle='--', linewidth=2)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "TfP59DZaNOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the full file path for 'results.csv' using the directory path and file name\n",
        "results_csv_path = os.path.join(post_training_files_path, 'results.csv')\n",
        "\n",
        "# Load the CSV file from the constructed path into a pandas DataFrame\n",
        "df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Remove any leading whitespace from the column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Plot the learning curves for each loss\n",
        "plot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Box Loss Learning Curve')\n",
        "plot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\n",
        "plot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "hUbVdMeSNOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Model Learning Curve Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The learning curves for box loss, classification loss, and distribution focal loss indicate a rapid decrease in loss values during the initial epochs, leveling off as training progresses. This trend, along with the close alignment of training and validation loss lines, suggests that the model is learning effectively without overfitting, meaning it is well-tuned to the dataset without being biased or too variable.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The smoothness of the learning curves, especially evident in the latter epochs, implies that the model is reaching a state of equilibrium, where additional training does not significantly enhance performance. This observation suggests that 100 epochs are sufficient for training this model, as further training is unlikely to result in substantial gains.\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "v3Z573cdNOkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Confusion_Matrix\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 5.2 |</span><span style='color:#141140'> Confusion Matrix Evaluation</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "ttlwJQMBNOkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Next, I will focus on displaying and meticulously analyzing the confusion matrix derived from our model's performance on the validation dataset:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "GkCe10VeNOkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the path to the normalized confusion matrix image\n",
        "confusion_matrix_path = os.path.join(post_training_files_path, 'confusion_matrix_normalized.png')\n",
        "\n",
        "# Read the image using cv2\n",
        "cm_img = cv2.imread(confusion_matrix_path)\n",
        "\n",
        "# Convert the image from BGR to RGB color space for accurate color representation with matplotlib\n",
        "cm_img = cv2.cvtColor(cm_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(10, 10), dpi=120)\n",
        "plt.imshow(cm_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "dIf3TAYlNOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Confusion Matrix Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The confusion matrix for our YOLOv8 vehicle detection model illustrates high accuracy as mentioned earlier as well. In 97% of instances, the model successfully identifies the presence of a vehicle when there is one, indicating strong detection capability. Conversely, in just 3% of cases, the model fails to detect a vehicle that is actually present, suggesting room for improvement in reducing false negatives.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "HH0eGqh4NOkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Performance_Metrics\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 5.3 |</span><span style='color:#141140'> Performance Metrics Assessment</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "_ksQJTBCNOkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Finally, I am delving into various metrics to understand the model's predictive accuracy and areas of potential improvement:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "6h9YHDGgNOkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the path to the best model weights file using os.path.join\n",
        "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
        "\n",
        "# Load the best model weights into the YOLO model\n",
        "best_model = YOLO(best_model_path)\n",
        "\n",
        "# Validate the best model using the validation set with default parameters\n",
        "metrics = best_model.val(split='val')"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "0TKdgxMhNOkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as can be seen in the above verbose"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "54yqUGDDNOkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
        "metrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n",
        "\n",
        "# Display the DataFrame\n",
        "metrics_df.round(3)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "vo2maezENOkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 Model Evaluation Insights</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        The YOLOv8 model shows impressive results on the validation set. With a precision of <b>91.6%</b>, it indicates that the majority of the predictions made by the model are correct. The recall score of <b>93.8%</b> demonstrates the model's ability to find most of the relevant cases in the dataset. The model's mean Average Precision (mAP) at 50% Intersection over Union (IoU) is <b>97.5%</b>, reflecting high accuracy in detecting objects with a considerable overlap with the ground truth. Even when the IoU threshold range is expanded from 50% to 95%, the model maintains a solid mAP of <b>74.2%</b>. Finally, the fitness score of <b>76.5%</b> indicates a good balance between precision, recall, and the IoU of the predictions, confirming the model's effectiveness in object detection tasks.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "YCvVmyhrNOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Model_Inference\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Model Inference & Generalization Assessment </p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "5TgcCl_iNOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To thoroughly assess our model's capability to generalize, I will conduct inferences in three distinct steps:\n",
        "    </p>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Inference on Validation Set Images</b></li>\n",
        "        <li><b>Inference on an Unseen Test Image</b></li>\n",
        "        <li><b>Inference on an Unseen Test Video</b></li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "VHiwe_IWNOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Inference_Validation\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 6.1 |</span><span style='color:#141140'> Inference on Validation Set Images</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "8CeuVub0NOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> First of all, I am going to evaluate model predictions on random images from the validation dataset:</p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "S_2yWRMjNOkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the validation images\n",
        "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
        "\n",
        "# List all jpg images in the directory\n",
        "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
        "\n",
        "# Select 9 images at equal intervals\n",
        "num_images = len(image_files)\n",
        "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
        "\n",
        "# Initialize the subplot\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
        "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
        "\n",
        "# Perform inference on each selected image and display it\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
        "    results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n",
        "    annotated_image = results[0].plot(line_width=1)\n",
        "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "    ax.imshow(annotated_image_rgb)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "UPAyQd65NOkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Inference_Test_Image\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 6.2 |</span><span style='color:#141140'> Inference on an Unseen Test Image</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "ZcFMZcH_NOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Now, I will employ the best version of our fine-tuned model to evaluate its generalization capabilities. I'll test it on the same image previously analyzed using the pre-trained YOLOv8 model on the COCO dataset:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "bdO6dRtTNOkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the image file\n",
        "sample_image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
        "\n",
        "# Perform inference on the provided image using best model\n",
        "results = best_model.predict(source=sample_image_path, imgsz=640, conf=0.7)\n",
        "\n",
        "# Annotate and convert image to numpy array\n",
        "sample_image = results[0].plot(line_width=2)\n",
        "\n",
        "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
        "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display annotated image\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(sample_image)\n",
        "plt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv8 Model', fontsize=20)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "yWTjybRsNOkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Enhanced Vehicle Detection with Fine-Tuning</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The comparison between the above image with the image we had in step 2 clearly demonstrates the benefits of fine-tuning the YOLOv8 model for a specialized task. In the image from step 2, the pre-trained model on the COCO dataset missed detecting a truck and misclassified it, indicating limitations when dealing with a specific class of objects due to its broader training scope. In contrast, the above image shows that the fine-tuned model on a vehicle-specific dataset has accurately detected and classified various vehicles, including the previously missed truck. This improvement highlights the model's enhanced capability to discern features specific to vehicles, leading to better precision and recall in vehicle detection.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "8ap56zCMNOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Inference_Test_Video\"></a>\n",
        "# <b><span style='color:#b2addb'>Step 6.3 |</span><span style='color:#141140'> Inference on an Unseen Test Video</span></b>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "iGv7LjCDNOkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Finally, I am going to assess the model's generalization capabilities on a completely new video, unseen during training. This step is crucial to demonstrate the model's ability to adapt and perform accurately in real-world applications, further solidifying its effectiveness outside of the controlled dataset environment:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "LIfc0oHpNOkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the sample video in the dataset\n",
        "dataset_video_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'\n",
        "\n",
        "# Define the destination path in the working directory\n",
        "video_path = '/kaggle/working/sample_video.mp4'\n",
        "\n",
        "# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\n",
        "shutil.copyfile(dataset_video_path, video_path)\n",
        "\n",
        "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
        "best_model.predict(source=video_path, save=True)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "scrolled": true,
        "tags": [],
        "trusted": true,
        "id": "KXtUhSVANOka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To ensure compatibility with various platforms, including Jupyter Notebooks, we'll convert the output <code>.avi</code> video file to the more universally supported <code>.mp4</code> format and then display it within the notebook environment:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "WRZGUCMLNOka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
        "!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/sample_video.avi processed_sample_video.mp4\n",
        "\n",
        "# Embed and display the processed sample video within the notebook\n",
        "Video(\"processed_sample_video.mp4\", embed=True, width=960)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "bz_DJ9kVNOka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Traffic_Intensity_Estimation\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Real-Time Traffic Intensity Estimation </p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "TyjhJyF5NOka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        As we transition into the practical application step of our project, we are set to deploy our finely-tuned vehicle detection model to analyze traffic density. This step is crucial in demonstrating the model's ability to generalize and perform accurately on unseen videos—videos that were not part of the model's training or validation sets.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "Our objective is to quantify the traffic by counting vehicles within specified areas on the road lanes, frame by frame. The analysis will not only reveal the vehicle count but also gauge the intensity of traffic, labeling it as 'Heavy' or 'Smooth' based on a predetermined threshold. The count and traffic flow insights are pivotal for urban planning and traffic management.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "While real-time processing on Kaggle is not viable, the code below simulates this process by modifying video frames, applying vehicle detection, and annotating the results. This mimics real-time analysis which can be achieved on local machines—even on CPUs—by processing webcam feeds or video files in real-time. The annotated video is then saved, ready to be reviewed for traffic assessment.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "Let's delve into the code that brings this all to life:\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "W-gM_UA5NOka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the threshold for considering traffic as heavy\n",
        "heavy_traffic_threshold = 10\n",
        "\n",
        "# Define the vertices for the quadrilaterals\n",
        "vertices1 = np.array([(465, 350), (609, 350), (510, 630), (2, 630)], dtype=np.int32)\n",
        "vertices2 = np.array([(678, 350), (815, 350), (1203, 630), (743, 630)], dtype=np.int32)\n",
        "\n",
        "# Define the vertical range for the slice and lane threshold\n",
        "x1, x2 = 325, 635\n",
        "lane_threshold = 609\n",
        "\n",
        "# Define the positions for the text annotations on the image\n",
        "text_position_left_lane = (10, 50)\n",
        "text_position_right_lane = (820, 50)\n",
        "intensity_position_left_lane = (10, 100)\n",
        "intensity_position_right_lane = (820, 100)\n",
        "\n",
        "# Define font, scale, and colors for the annotations\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 1\n",
        "font_color = (255, 255, 255)    # White color for text\n",
        "background_color = (0, 0, 255)  # Red background for text\n",
        "\n",
        "# Open the video\n",
        "cap = cv2.VideoCapture('sample_video.mp4')\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('traffic_density_analysis.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Read until video is completed\n",
        "while cap.isOpened():\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        # Create a copy of the original frame to modify\n",
        "        detection_frame = frame.copy()\n",
        "\n",
        "        # Black out the regions outside the specified vertical range\n",
        "        detection_frame[:x1, :] = 0  # Black out from top to x1\n",
        "        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n",
        "\n",
        "        # Perform inference on the modified frame\n",
        "        results = best_model.predict(detection_frame, imgsz=640, conf=0.4)\n",
        "        processed_frame = results[0].plot(line_width=1)\n",
        "\n",
        "        # Restore the original top and bottom parts of the frame\n",
        "        processed_frame[:x1, :] = frame[:x1, :].copy()\n",
        "        processed_frame[x2:, :] = frame[x2:, :].copy()\n",
        "\n",
        "        # Draw the quadrilaterals on the processed frame\n",
        "        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "\n",
        "        # Retrieve the bounding boxes from the results\n",
        "        bounding_boxes = results[0].boxes\n",
        "\n",
        "        # Initialize counters for vehicles in each lane\n",
        "        vehicles_in_left_lane = 0\n",
        "        vehicles_in_right_lane = 0\n",
        "\n",
        "        # Loop through each bounding box to count vehicles in each lane\n",
        "        for box in bounding_boxes.xyxy:\n",
        "            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n",
        "            if box[0] < lane_threshold:\n",
        "                vehicles_in_left_lane += 1\n",
        "            else:\n",
        "                vehicles_in_right_lane += 1\n",
        "\n",
        "        # Determine the traffic intensity for the left lane\n",
        "        traffic_intensity_left = \"Heavy\" if vehicles_in_left_lane > heavy_traffic_threshold else \"Smooth\"\n",
        "        # Determine the traffic intensity for the right lane\n",
        "        traffic_intensity_right = \"Heavy\" if vehicles_in_right_lane > heavy_traffic_threshold else \"Smooth\"\n",
        "\n",
        "\n",
        "        # Add a background rectangle for the left lane vehicle count\n",
        "        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25),\n",
        "                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n",
        "\n",
        "        # Add the vehicle count text on top of the rectangle for the left lane\n",
        "        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane,\n",
        "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
        "\n",
        "        # Add a background rectangle for the left lane traffic intensity\n",
        "        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25),\n",
        "                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n",
        "\n",
        "        # Add the traffic intensity text on top of the rectangle for the left lane\n",
        "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane,\n",
        "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
        "\n",
        "        # Add a background rectangle for the right lane vehicle count\n",
        "        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25),\n",
        "                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n",
        "\n",
        "        # Add the vehicle count text on top of the rectangle for the right lane\n",
        "        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane,\n",
        "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
        "\n",
        "        # Add a background rectangle for the right lane traffic intensity\n",
        "        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25),\n",
        "                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n",
        "\n",
        "        # Add the traffic intensity text on top of the rectangle for the right lane\n",
        "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane,\n",
        "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n",
        "        # cv2.imshow('Real-time Analysis', processed_frame)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n",
        "        #     break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all the frames\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "scrolled": true,
        "tags": [],
        "trusted": true,
        "id": "pPEZeAEnNOka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Finally, lets convert the output <code>.avi</code> video to <code>.mp4</code> format for notebook playback and display it:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "6HhiRvuiNOkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n",
        "!ffmpeg -y -loglevel panic -i /kaggle/working/traffic_density_analysis.avi traffic_density_analysis.mp4\n",
        "\n",
        "# Embed and display the processed sample video within the notebook\n",
        "Video(\"traffic_density_analysis.mp4\", embed=True, width=960)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "KVTouk2kNOkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Model_Export\"></a>\n",
        "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 8 | Model Export for Cross-Platform Deployment </p>\n",
        "⬆️ [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "49t_vBewNOkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        We're now at the final step of our project – exporting our best model for future use and deployment. Currently, our model exists in a <code>.pt</code> format, optimal for tasks such as ongoing training, additional fine-tuning, or deployment within PyTorch-compatible environments. To enhance the model's versatility and ensure it can be utilized across various platforms, we'll be exporting it in the ONNX (Open Neural Network Exchange) format. ONNX is specially designed for model portability, enabling seamless operation across different machine learning frameworks including PyTorch, TensorFlow, and Microsoft's Cognitive Toolkit (CNTK), as well as compatibility with specialized hardware accelerators. This step ensures our model's readiness for diverse deployment scenarios, aligning with the broad scope of modern AI applications. For more details on the export process and available formats, refer to <a href=\"https://docs.ultralytics.com/modes/export/#arguments\">Ultralytics Documentation</a>.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "4hc6PRSYNOkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model\n",
        "best_model.export(format='onnx')"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "id": "X_ffHjPTNOkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #eae8fa; font-size: 120%; text-align: center;\">\n",
        "    <strong>\n",
        "        🌐 For comprehensive insights, extensive code, and additional resources, visit the project's\n",
        "        <a href=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/tree/master\" style=\"color: #141140; text-decoration: none;\">\n",
        "            <em><u>GitHub Repository</u></em>\n",
        "        </a>\n",
        "        🌐\n",
        "    </strong>\n",
        "</div>\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "9qziy1BbNOkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"left\"><font color=#141140>Best Regards</font></h2>"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "VmaEh4ZdNOkc"
      }
    }
  ]
}