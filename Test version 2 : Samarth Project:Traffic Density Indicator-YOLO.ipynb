{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7166601,"sourceType":"datasetVersion","datasetId":4107330}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":68.691048,"end_time":"2023-12-12T12:45:53.82776","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-12T12:44:45.136712","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/radhakrishnanrajan/samarth-project-traffic-density-indicator-yolo?scriptVersionId=173463927\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Student Samarth Gupta Project :: Traffic Density Estimation Indicator Project 🚦 🚦 </b></h1>\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        <b>Overview : </b>Leveraging YOLO's real-time detection capabilities, this project focuses on <strong>Traffic Density Estimation Indicator</strong>, a vital component in urban and traffic management. The goal is to count vehicles within a specific area in each frame to assess traffic density. This valuable data aids in identifying peak traffic periods, congested zones, and assists in urban planning. Through this project, we aim to develop a comprehensive toolset that provides detailed insights into traffic flow and patterns, enhancing traffic management and city planning strategies.\n    </p>\n</div>\n","metadata":{"papermill":{"duration":0.017002,"end_time":"2023-12-12T12:44:48.870172","exception":false,"start_time":"2023-12-12T12:44:48.85317","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Project Objectives</b></h1>\n    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n        <li><strong>YOLOv8 Model Selection and Initial Assessment:</strong> Starting with YOLOv8's pre-trained model selection, assessing its initial performance on COCO dataset for vehicle detection.</li>\n        <li><strong>Specialized Vehicle Dataset Preparation:</strong> Curating and annotating a vehicle-specific dataset to refine the model's detection capabilities for diverse vehicle types.</li>\n        <li><strong>Model Fine-Tuning for Enhanced Vehicle Detection:</strong> Employing transfer learning to fine-tune the YOLOv8 model, focusing on vehicle detection from aerial perspectives for improved precision and recall.</li>\n        <li><strong>Comprehensive Model Performance Evaluation:</strong> Analyzing learning curves, evaluating confusion matrix, and assessing performance metrics to validate the model's accuracy and generalization capabilities.</li>\n        <li><strong>Inference and Generalization on Test Data:</strong> Testing the model's generalization on validation images, an unseen test image, and a test video to demonstrate its practical application and effectiveness.</li>\n        <li><strong>Real-Time Traffic Density Estimation:</strong> Implementing an algorithm to estimate traffic density by counting vehicles and analyzing traffic intensity in real-time on test video data.</li>\n        <li><strong>Cross-Platform Model Deployment Preparation:</strong> Exporting the fine-tuned model in ONNX format for versatile use across different platforms and environments.</li>\n    </ul>\n</div>\n","metadata":{"papermill":{"duration":0.016939,"end_time":"2023-12-12T12:44:48.904292","exception":false,"start_time":"2023-12-12T12:44:48.887353","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Initialization\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 1 | Setup and Initialization</p>","metadata":{"papermill":{"duration":0.016921,"end_time":"2023-12-12T12:44:49.006952","exception":false,"start_time":"2023-12-12T12:44:48.990031","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Install Ultralytics library\n!pip install ultralytics","metadata":{"_kg_hide-output":true,"papermill":{"duration":13.887399,"end_time":"2023-12-12T12:45:02.989658","exception":false,"start_time":"2023-12-12T12:44:49.102259","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -U ray[tune]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -U ipywidgets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Then, import all the essential libraries needed for the project:</p>\n</div>","metadata":{"papermill":{"duration":0.018229,"end_time":"2023-12-12T12:45:03.027025","exception":false,"start_time":"2023-12-12T12:45:03.008796","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Disable warnings in the notebook to maintain clean output cells\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import necessary libraries\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport yaml\nfrom PIL import Image\nfrom ultralytics import YOLO\nfrom IPython.display import Video\nprint(\"Import Completed Sucessfully\")","metadata":{"papermill":{"duration":5.616125,"end_time":"2023-12-12T12:45:08.66149","exception":false,"start_time":"2023-12-12T12:45:03.045365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure the visual appearance of Seaborn plots\nsns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')\nprint(\"Setup Completed Sucessfully\")","metadata":{"papermill":{"duration":0.027946,"end_time":"2023-12-12T12:45:08.709398","exception":false,"start_time":"2023-12-12T12:45:08.681452","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Here are the pre-trained YOLOv8 object detection models, which have been trained on the <strong>COCO dataset</strong>. The Common Objects in Context (COCO) dataset is extensive, designed for object detection, segmentation, and captioning, and encompasses <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80 diverse object categories</a>:</p>\n</div>","metadata":{"papermill":{"duration":0.018944,"end_time":"2023-12-12T12:45:08.785891","exception":false,"start_time":"2023-12-12T12:45:08.766947","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/YOLOv8_object_detection_models.jpg?raw=true\" width=\"2400\">","metadata":{"papermill":{"duration":0.018452,"end_time":"2023-12-12T12:45:08.823299","exception":false,"start_time":"2023-12-12T12:45:08.804847","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Model Performance Trade-offs</b></h1>\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The YOLOv8 suite presents five distinct models: <strong>nano</strong>, <strong>small</strong>, <strong>medium</strong>, <strong>large</strong>, and <strong>xlarge</strong>. A clear trend emerges from the data: as model size increases, there's a notable improvement in <strong>mAP</strong>, indicating enhanced accuracy. Conversely, this augmentation comes at the cost of speed, with larger models being slower. All models adhere to a standard input size of <strong>640x640</strong> pixels, optimizing performance across diverse applications.\n    </p>\n</div>","metadata":{"papermill":{"duration":0.018195,"end_time":"2023-12-12T12:45:08.860376","exception":false,"start_time":"2023-12-12T12:45:08.842181","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">For  real-time traffic density estimation application, we select the <strong>YOLOv8 nano pre-trained model (yolov8n.pt)</strong> to handle vehicle detection. This model ensures the fastest possible inference time, making it well-suited for real-time use:</p>\n</div>","metadata":{"papermill":{"duration":0.018603,"end_time":"2023-12-12T12:45:08.935075","exception":false,"start_time":"2023-12-12T12:45:08.916472","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load a pretrained YOLOv8n model from Ultralytics\nmodel = YOLO('yolov8n.pt')","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.536317,"end_time":"2023-12-12T12:45:09.490231","exception":false,"start_time":"2023-12-12T12:45:08.953914","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">The pre-trained model we've loaded is trained on the COCO dataset, which includes the 'car' and 'truck' classes among its 80 different categories — exactly what we need for our project. Now, put the model to the test and see how it performs on a sample image:</p>\n</div>","metadata":{"papermill":{"duration":0.018601,"end_time":"2023-12-12T12:45:09.528765","exception":false,"start_time":"2023-12-12T12:45:09.510164","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Path to the image file\nimage_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n\n# Perform inference on the provided image(s)\nresults = model.predict(source=image_path, \n                        imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n                        conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n\n# Annotate and convert image to numpy array\nsample_image = results[0].plot(line_width=2)\n\n# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\n# Display annotated image\nplt.figure(figsize=(20,15))\nplt.imshow(sample_image)\nplt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=20)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Pre-trained Model Detection Analysis</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        In the sample image, the pre-trained model missed the detectable truck and car that were clearly visible. A model pre-trained on a dataset with a broad range of classes, like COCO's 80 different categories, may not perform as well on a specific subset of those categories due to the diversity of objects it has been trained to recognize. If we fine-tune this model on a specialized dataset that focuses solely on vehicles, it can learn to detect various types of vehicles more accurately. Fine-tuning on a vehicle-specific dataset allows the model to become more specialized, adjusting the weights to be more sensitive to features specific to vehicles. As a result, the model's mean Average Precision (mAP) for vehicle detection could improve because it's being optimized on a narrower, more relevant range of classes for our specific application. Fine-tuning also helps the model generalize better for vehicle detection tasks, potentially reducing false negatives (like missing a detectable truck) and improving overall detection performance.\n    </p>\n</div>\n","metadata":{"papermill":{"duration":0.04642,"end_time":"2023-12-12T12:45:18.611612","exception":false,"start_time":"2023-12-12T12:45:18.565192","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Dataset_Exploration\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Dataset Exploration</p>","metadata":{"papermill":{"duration":0.046519,"end_time":"2023-12-12T12:45:18.70644","exception":false,"start_time":"2023-12-12T12:45:18.659921","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Dataset Preparation for Model Fine-tuning</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        To fine-tune the pre-trained model on a specialized dataset that focuses solely on vehicles, so that it can learn to detect various types of vehicles more accurately, I have prepared a dataset which is available at this link <a href=\"https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset\">Top-View Vehicle Detection Image Dataset for YOLOv8</a>. The dataset zeroes in on the 'Vehicle' class, covering a wide variety of vehicles such as cars, trucks, and buses. It is composed of 626 images sourced from top-view perspectives, <strong>annotated meticulously in the YOLOv8 format</strong> for effective vehicle detection.\n    </p>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The dataset undergoes a standardization process where each image is resized to a uniform resolution of <strong>640x640 pixels</strong>. To bolster the model's ability to generalize, augmentations were applied to the training data, which consists of 536 images. The validation set contains 90 images and remains unaugmented to preserve the integrity of performance evaluation.\n    </p>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        This dataset, curated from <a href=\"https://www.pexels.com/search/videos/\">Pexels</a>, captures the diversity of vehicles from an aerial view, making it ideal for highway monitoring tasks. Each video frame was selected at a sampling rate of 1 frame per second using <a href=\"https://universe.roboflow.com/farzad/vehicle_detection_yolov8\">Roboflow</a>, which facilitated precise annotation for object detection.\n    </p>\n</div>","metadata":{"papermill":{"duration":0.04706,"end_time":"2023-12-12T12:45:18.800921","exception":false,"start_time":"2023-12-12T12:45:18.753861","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Next begin  exploration by examining the '<strong>data.yaml</strong>' file:</p>\n</div>\n","metadata":{"papermill":{"duration":0.046651,"end_time":"2023-12-12T12:45:18.986753","exception":false,"start_time":"2023-12-12T12:45:18.940102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define the dataset_path\ndataset_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset'\n\n# Set the path to the YAML file\nyaml_file_path = os.path.join(dataset_path, 'data.yaml')\n\n# Load and print the contents of the YAML file\nwith open(yaml_file_path, 'r') as file:\n    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n    print(yaml.dump(yaml_content, default_flow_style=False))","metadata":{"papermill":{"duration":0.066253,"end_time":"2023-12-12T12:45:19.099731","exception":false,"start_time":"2023-12-12T12:45:19.033478","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Now, continue  exploration by counting the images in both the training and validation sets and verifying their sizes:</p>\n</div>","metadata":{"papermill":{"duration":0.046625,"end_time":"2023-12-12T12:45:19.287516","exception":false,"start_time":"2023-12-12T12:45:19.240891","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set paths for training and validation image sets\ntrain_images_path = os.path.join(dataset_path, 'train', 'images')\nvalid_images_path = os.path.join(dataset_path, 'valid', 'images')\n\n# Initialize counters for the number of images\nnum_train_images = 0\nnum_valid_images = 0\n\n# Initialize sets to hold the unique sizes of images\ntrain_image_sizes = set()\nvalid_image_sizes = set()\n\n# Check train images sizes and count\nfor filename in os.listdir(train_images_path):\n    if filename.endswith('.jpg'):  \n        num_train_images += 1\n        image_path = os.path.join(train_images_path, filename)\n        with Image.open(image_path) as img:\n            train_image_sizes.add(img.size)\n\n# Check validation images sizes and count\nfor filename in os.listdir(valid_images_path):\n    if filename.endswith('.jpg'): \n        num_valid_images += 1\n        image_path = os.path.join(valid_images_path, filename)\n        with Image.open(image_path) as img:\n            valid_image_sizes.add(img.size)\n\n# Print the results\nprint(f\"Number of training images: {num_train_images}\")\nprint(f\"Number of validation images: {num_valid_images}\")\n\n# Check if all images in training set have the same size\nif len(train_image_sizes) == 1:\n    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\nelse:\n    print(\"Training images have varying sizes.\")\n\n# Check if all images in validation set have the same size\nif len(valid_image_sizes) == 1:\n    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\nelse:\n    print(\"Validation images have varying sizes.\")","metadata":{"papermill":{"duration":4.275063,"end_time":"2023-12-12T12:45:23.611095","exception":false,"start_time":"2023-12-12T12:45:19.336032","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Dataset Analysis Insights</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The dataset for the project consists of 536 training images and 90 validation images, all uniformly sized at 640x640 pixels. This size aligns with the benchmark standard for the YOLOv8 model, ensuring optimal accuracy and speed during model performance. The split ratio of approximately 85% for training and 15% for validation provides a substantial amount of data for model learning while retaining enough images for effective model validation.\n    </p>\n</div>","metadata":{"papermill":{"duration":0.051006,"end_time":"2023-12-12T12:45:23.712196","exception":false,"start_time":"2023-12-12T12:45:23.66119","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Take a look at a few images from the training dataset to get a sense of what the data looks like:</p>\n</div>","metadata":{"papermill":{"duration":0.047477,"end_time":"2023-12-12T12:45:23.809043","exception":false,"start_time":"2023-12-12T12:45:23.761566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# List all jpg images in the directory\nimage_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n\n# Select 8 images at equal intervals\nnum_images = len(image_files)\nselected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n\n# Create a 2x4 subplot\nfig, axes = plt.subplots(2, 4, figsize=(20, 11))\n\n# Display each of the selected images\nfor ax, img_file in zip(axes.ravel(), selected_images):\n    img_path = os.path.join(train_images_path, img_file)\n    image = Image.open(img_path)\n    ax.imshow(image)\n    ax.axis('off')  \n\nplt.suptitle('Sample Images from Training Dataset', fontsize=20)\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":2.758546,"end_time":"2023-12-12T12:45:26.614041","exception":false,"start_time":"2023-12-12T12:45:23.855495","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Fine_Tuning_YOLOv8\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Fine-Tuning YOLOv8 </p>","metadata":{"papermill":{"duration":0.095762,"end_time":"2023-12-12T12:45:26.808073","exception":false,"start_time":"2023-12-12T12:45:26.712311","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step of the project, we're set to fine-tune the YOLOv8 pre-trained object detection model using transfer learning, specifically tailoring it to the 'Top-View Vehicle Detection Image Dataset'. By leveraging the YOLOv8 model's existing weights from its training on the comprehensive COCO dataset, we start from a robust foundation rather than from scratch. This approach not only saves significant time and resces but also capitalizes on  focused dataset to enhance the model's ability to accurately recognize and detect vehicles in top-view images. This method of training enables efficient and effective model adaptation, ensuring it's finely attuned to the specificities of vehicle detection from aerial perspectives:</p>\n</div>","metadata":{"papermill":{"duration":0.0987,"end_time":"2023-12-12T12:45:27.003546","exception":false,"start_time":"2023-12-12T12:45:26.904846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train the model on  custom dataset\nresults = model.train(\n    data=yaml_file_path,     # Path to the dataset configuration file\n    epochs=10,              # Number of epochs to train for\n    imgsz=640,               # Size of input images as integer\n    device=0,                # Device to run on, i.e. cuda device=0 \n    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n    batch=32,                # Number of images per batch\n    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n    lr0=0.0001,              # Initial learning rate \n    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n    dropout=0.1,             # Use dropout regularization\n    seed=0                   # Random seed for reproducibility\n)","metadata":{"papermill":{"duration":23.627724,"end_time":"2023-12-12T12:45:50.728563","exception":true,"start_time":"2023-12-12T12:45:27.100839","status":"failed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Understanding Run Summary Metrics</b></h2>\n    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n        <li><b>Learning Rate per Group (lr/pg0, lr/pg1, lr/pg2):</b> These values represent the learning rate for different groups of layers in the neural network. A lower learning rate means the model updates its weights more slowly during training. Consistent learning rates across groups indicate uniform adjustments during the learning process.</li>\n        <li><b>Mean Average Precision at 50% IoU (metrics/mAP50(B)):</b> This metric measures the model's accuracy in detecting objects with at least 50% Intersection over Union (IoU) with ground truth. A score of 0.97 suggests the model is highly accurate at this IoU threshold.</li>\n        <li><b>Mean Average Precision across IoU from 50% to 95% (metrics/mAP50-95(B)):</b> This is an average of mAP calculated at different IoU thresholds, from 50% to 95%. A score of 0.74 indicates good overall accuracy across these varying thresholds.</li>\n        <li><b>Precision (metrics/precision(B)):</b> Precision measures the ratio of correctly predicted positive observations to the total predicted positives. A score of 0.92 means the model is highly precise in its predictions.</li>\n        <li><b>Recall (metrics/recall(B)):</b> Recall calculates the ratio of correctly predicted positive observations to all observations in actual class. A recall of 0.94 shows the model is very good at finding all relevant cases within the dataset.</li>\n        <li><b>Model Computational Complexity (model/GFLOPs):</b> Indicates the model's computational demands, with the GFLOPs value suggesting moderate complexity.</li>\n        <li><b>Model Parameters:</b> This is the total number of trainable parameters in the model. Almost 3 million parameters indicate a model of moderate size and complexity.</li>\n        <li><b>Inference Speed (model/speed_PyTorch(ms)):</b> The time taken for the model to make a single prediction (inference). 4.6 ms is quite fast, which is good for real-time applications.</li>\n        <li><b>Training Losses (train/box_loss, train/cls_loss, train/dfl_loss):</b>These are different types of losses during training. 'box_loss' refers to the error in bounding box predictions, 'cls_loss' to classification error, and 'dfl_loss' to distribution focal loss. Lower values indicate better performance.</li>\n        <li><b>Validation Losses (val/box_loss, val/cls_loss, val/dfl_loss):</b> Similar to training losses, these are losses calculated on the validation dataset. They give an idea of how well the model generalizes to new, unseen data. Almost similar loss values for both training and validation indicate that the model is not overfitting.</li>\n    </ul>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Model_Performance_Evaluation\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Model Performance Evaluation </p>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Post-training,  model generates several files and folders that encapsulate various aspects of the training run. See the list of generated files:</p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define the path to the directory\npost_training_files_path = '/kaggle/working/runs/detect/train'\n\n# List the files in the directory\n!ls {post_training_files_path}","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📁 Training Output Files Explained</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        Here’s a rundown of each item:\n    </p>\n    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n        <li><b>Weights Folder:</b> Contains the 'best.pt' and 'last.pt' files, which are the best and most recent weights of  trained model respectively.</li>\n        <li><b>Args:</b> A file that stores the arguments or parameters that were used during the training process.</li>\n        <li><b>Confusion Matrix:</b> Visual representations of the model performance. One is normalized, which helps in understanding the true positive rate across classes.</li>\n        <li><b>Events File:</b> Contains logs of events that occurred during training, useful for debugging and analysis.</li>\n        <li><b>F1 Curve:</b> Illustrates the F1 score of the model over time, balancing precision and recall.</li>\n        <li><b>Labels:</b> Shows the distribution of different classes within the dataset and their correlation.</li>\n        <li><b>P Curve, PR Curve, R Curve:</b> These are Precision, Precision-Recall, and Recall curves, respectively, providing insights into the trade-offs between different metrics at various thresholds.</li>\n        <li><b>results:</b> This csv file captures a comprehensive set of performance metrics recorded at each epoch during the model's training process.</li>\n        <li><b>Train Batch Images:</b> Sample images from the training set with model predictions overlaid, useful for a quick visual check of model performance.</li>\n        <li><b>Validation Batch Images:</b> Similar to train batch images, these are from the validation set and include both labels and predictions, providing a glimpse into how well the model generalizes.</li>\n    </ul>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        Comprehensive evaluation and analysis of  model's performance, which involves:\n    </p>\n    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n        <li><b>Learning Curves Analysis</b></li>\n        <li><b>Confusion Matrix Evaluation</b></li>\n        <li><b>Performance Metrics Assessment</b></li>\n    </ul>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Learning_Curves\"></a>\n# <b><span style='color:#b2addb'>Step 5.1 |</span><span style='color:#141140'> Learning Curves Analysis</span></b>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step, we review the training and validation loss trends over epochs to assess the learning stability and efficiency:</p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define a function to plot learning curves for loss values\ndef plot_learning_curve(df, train_loss_col, val_loss_col, title):\n    plt.figure(figsize=(12, 5))\n    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='#141140', linestyle='-', linewidth=2)\n    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='orangered', linestyle='--', linewidth=2)\n    plt.title(title)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the full file path for 'results.csv' using the directory path and file name\nresults_csv_path = os.path.join(post_training_files_path, 'results.csv')\n\n# Load the CSV file from the constructed path into a pandas DataFrame\ndf = pd.read_csv(results_csv_path)\n\n# Remove any leading whitespace from the column names\ndf.columns = df.columns.str.strip()\n\n# Plot the learning curves for each loss\nplot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Box Loss Learning Curve')\nplot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\nplot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy for each loss\ndf['train/box_accuracy'] = 1 - df['train/box_loss']\ndf['val/box_accuracy'] = 1 - df['val/box_loss']\n\ndf['train/cls_accuracy'] = 1 - df['train/cls_loss']\ndf['val/cls_accuracy'] = 1 - df['val/cls_loss']\n\ndf['train/dfl_accuracy'] = 1 - df['train/dfl_loss']\ndf['val/dfl_accuracy'] = 1 - df['val/dfl_loss']\n\n# Plot the learning curves for each accuracy\nplot_learning_curve(df, 'train/box_accuracy', 'val/box_accuracy', 'Box Accuracy Learning Curve')\nplot_learning_curve(df, 'train/cls_accuracy', 'val/cls_accuracy', 'Classification Accuracy Learning Curve')\nplot_learning_curve(df, 'train/dfl_accuracy', 'val/dfl_accuracy', 'Distribution Focal Accuracy Learning Curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Model Learning Curve Analysis</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The learning curves for box loss, classification loss, and distribution focal loss indicate a rapid decrease in loss values during the initial epochs, leveling off as training progresses. This trend, along with the close alignment of training and validation loss lines, suggests that the model is learning effectively without overfitting, meaning it is well-tuned to the dataset without being biased or too variable.\n    </p>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The smoothness of the learning curves, especially evident in the latter epochs, implies that the model is reaching a state of equilibrium, where additional training does not significantly enhance performance. This observation suggests that 100 epochs are sufficient for training this model, as further training is unlikely to result in substantial gains.\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Performance_Metrics\"></a>\n# <b><span style='color:#b2addb'>Step 5.1b |</span><span style='color:#141140'> Performance Metrics Assessment</span></b>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Finally, we are delving into various metrics to understand the model's predictive accuracy and areas of potential improvement:</p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Construct the path to the best model weights file using os.path.join\nbest_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n\n# Load the best model weights into the YOLO model\nbest_model = YOLO(best_model_path)\n\n# Validate the best model using the validation set with default parameters\nmetrics = best_model.val(split='val')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as can be seen in the above verbose","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Convert the dictionary to a pandas DataFrame and use the keys as the index\nmetrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n\n# Display the DataFrame\nmetrics_df.round(3)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Model Evaluation Insights</b></h1>\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n        The YOLOv8 model shows impressive results on the validation set. With a precision of <b>89.6%</b>, it indicates that the majority of the predictions made by the model are correct. The recall score of <b>89.6%</b> demonstrates the model's ability to find most of the relevant cases in the dataset. The model's mean Average Precision (mAP) at 50% Intersection over Union (IoU) is <b>95.6%</b>, reflecting high accuracy in detecting objects with a considerable overlap with the ground truth. Even when the IoU threshold range is expanded from 50% to 95%, the model maintains a solid mAP of <b>68.7%</b>. Finally, the fitness score of <b>71.4%</b> indicates a good balance between precision, recall, and the IoU of the predictions, confirming the model's effectiveness in object detection tasks.\n    </p>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Model_Inference\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Model Inference & Generalization Assessment </p>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        To thoroughly assess the model's capability to generalize, we will conduct inferences in three distinct steps:\n    </p>\n    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n        <li><b>Inference on Validation Set Images</b></li>\n        <li><b>Inference on an Unseen Test Image</b></li>\n        <li><b>Inference on an Unseen Test Video</b></li>\n    </ul>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Inference_Validation\"></a>\n# <b><span style='color:#b2addb'>Step 6.1 |</span><span style='color:#141140'> Inference on Validation Set Images</span></b>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> First  evaluate model predictions on random images from the validation dataset:</p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define the path to the validation images\nvalid_images_path = os.path.join(dataset_path, 'valid', 'images')\n\n# List all jpg images in the directory\nimage_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n\n# Select 9 images at equal intervals\nnum_images = len(image_files)\nselected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n\n# Initialize the subplot\nfig, axes = plt.subplots(3, 3, figsize=(20, 21))\nfig.suptitle('Validation Set Inferences', fontsize=24)\n\n# Perform inference on each selected image and display it\nfor i, ax in enumerate(axes.flatten()):\n    image_path = os.path.join(valid_images_path, selected_images[i])\n    results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n    annotated_image = results[0].plot(line_width=1)\n    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n    ax.imshow(annotated_image_rgb)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Inference_Test_Image\"></a>\n# <b><span style='color:#b2addb'>Step 6.2 |</span><span style='color:#141140'> Inference on an Unseen Test Image</span></b>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        Next we employ the best version of  fine-tuned model to evaluate its generalization capabilities. I'll test it on the same image previously analyzed using the pre-trained YOLOv8 model on the COCO dataset:\n    </p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Path to the image file\nsample_image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n\n# Perform inference on the provided image using best model\nresults = best_model.predict(source=sample_image_path, imgsz=640, conf=0.7) \n                        \n# Annotate and convert image to numpy array\nsample_image = results[0].plot(line_width=2)\n\n# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\n# Display annotated image\nplt.figure(figsize=(20,15))\nplt.imshow(sample_image)\nplt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv8 Model', fontsize=20)\nplt.axis('off')\nplt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>Enhanced Vehicle Detection with Fine-Tuning</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The comparison between the above image with the image we had in step 2 clearly demonstrates the benefits of fine-tuning the YOLOv8 model for a specialized task. In the image from step 2, the pre-trained model on the COCO dataset missed detecting a truck and misclassified it, indicating limitations when dealing with a specific class of objects due to its broader training scope. In contrast, the above image shows that the fine-tuned model on a vehicle-specific dataset has accurately detected and classified various vehicles, including the previously missed truck. This improvement highlights the model's enhanced capability to discern features specific to vehicles, leading to better precision and recall in vehicle detection.\n    </p>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"Inference_Test_Video\"></a>\n# <b><span style='color:#b2addb'>Step 6.3 |</span><span style='color:#141140'> Inference on an Unseen Test Video</span></b>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        Final Testing to assess the model's generalization capabilities on a completely new video, unseen during training. This step is crucial to demonstrate the model's ability to adapt and perform accurately in real-world applications, further solidifying its effectiveness outside of the controlled dataset environment:\n    </p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define the path to the sample video in the dataset\ndataset_video_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'\n\n# Define the destination path in the working directory\nvideo_path = '/kaggle/working/sample_video.mp4'\n\n# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\nshutil.copyfile(dataset_video_path, video_path)\n\n# Initiate vehicle detection on the sample video using the best performing model and save the output\nbest_model.predict(source=video_path, save=True)","metadata":{"_kg_hide-output":true,"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        To ensure compatibility with various platforms, including Jupyter Notebooks, we convert the output <code>.avi</code> video file to the more universally supported <code>.mp4</code> format and then display it within the notebook environment:\n    </p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/sample_video.avi processed_sample_video.mp4\n\n# Embed and display the processed sample video within the notebook\nVideo(\"processed_sample_video.mp4\", embed=True, width=960)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Traffic_Intensity_Estimation\"></a>\n# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Results of Traffic Intensity Estimation Indicator </p>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        As we transition into the practical application step of  project, we are set to deploy  finely-tuned vehicle detection model to analyze traffic density. This step is crucial in demonstrating the model's ability to generalize and perform accurately on unseen videos—videos that were not part of the model's training or validation sets.\n    </p>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n objective is to quantify the traffic by counting vehicles within specified areas on the road lanes, frame by frame. The analysis will not only reveal the vehicle count but also gauge the intensity of traffic, labeling it as 'Heavy' or 'Smooth' based on a predetermined threshold. The count and traffic flow insights are pivotal for urban planning and traffic management.\n    </p>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\nWhile real-time processing on Kaggle is not viable, the code below simulates this process by modifying video frames, applying vehicle detection, and annotating the results. This mimics real-time analysis which can be achieved on local machines—even on CPUs—by processing webcam feeds or video files in real-time. The annotated video is then saved, ready to be reviewed for traffic assessment.\n    </p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\nWe delve into the code that brings this all to life:\n</div>","metadata":{}},{"cell_type":"code","source":"# Define the threshold for considering traffic as heavy\nheavy_traffic_threshold = 3\n\n# Define the vertices for the quadrilaterals\nvertices1 = np.array([(465, 350), (609, 350), (510, 630), (2, 630)], dtype=np.int32)\nvertices2 = np.array([(678, 350), (815, 350), (1203, 630), (743, 630)], dtype=np.int32)\n\n# Define the vertical range for the slice and lane threshold\nx1, x2 = 325, 635 \nlane_threshold = 609\n\n# Define the positions for the text annotations on the image\ntext_position_left_lane = (10, 50)\ntext_position_right_lane = (820, 50)\nintensity_position_left_lane = (10, 100)\nintensity_position_right_lane = (820, 100)\n\n# Define font, scale, and colors for the annotations\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfont_scale = 1\nfont_color = (255, 255, 255)    # White color for text\nbackground_color = (0, 0, 255)  # Red background for text\n        \n# Open the video\ncap = cv2.VideoCapture('sample_video.mp4')\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('traffic_density_analysis.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n# Read until video is completed\nwhile cap.isOpened():\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n    if ret:\n        # Create a copy of the original frame to modify\n        detection_frame = frame.copy()\n    \n        # Black out the regions outside the specified vertical range\n        detection_frame[:x1, :] = 0  # Black out from top to x1\n        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n        \n        # Perform inference on the modified frame\n        results = best_model.predict(detection_frame, imgsz=640, conf=0.4)\n        processed_frame = results[0].plot(line_width=1)\n        \n        # Restore the original top and bottom parts of the frame\n        processed_frame[:x1, :] = frame[:x1, :].copy()\n        processed_frame[x2:, :] = frame[x2:, :].copy()        \n        \n        # Draw the quadrilaterals on the processed frame\n        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(0, 255, 0), thickness=2)\n        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n        \n        # Retrieve the bounding boxes from the results\n        bounding_boxes = results[0].boxes\n\n        # Initialize counters for vehicles in each lane\n        vehicles_in_left_lane = 0\n        vehicles_in_right_lane = 0\n        output_result = 0\n\n        # Loop through each bounding box to count vehicles in each lane\n        for box in bounding_boxes.xyxy:\n            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n            if box[0] < lane_threshold:\n                vehicles_in_left_lane += 1\n            else:\n                vehicles_in_right_lane += 1\n                \n        # Determine the traffic intensity for the left lane\n        traffic_intensity_left = \"Enable Left Lane 1\" if vehicles_in_left_lane > heavy_traffic_threshold else \"Normal Traffic\"\n        # Determine the traffic intensity for the right lane\n        traffic_intensity_right = \"Enable Left Lane 2\" if vehicles_in_right_lane > heavy_traffic_threshold else \"Normal Traffic\"\n\n        \n        # Add a background rectangle for the left lane vehicle count\n        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25), \n                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n\n        # Add the vehicle count text on top of the rectangle for the left lane\n        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the left lane traffic intensity\n        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25), \n                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n\n        # Add the traffic intensity text on top of the rectangle for the left lane\n        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the right lane vehicle count\n        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25), \n                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n\n        # Add the vehicle count text on top of the rectangle for the right lane\n        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the right lane traffic intensity\n        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25), \n                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n\n        # Add the traffic intensity text on top of the rectangle for the right lane\n        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Write the processed frame to the output video\n        out.write(processed_frame)\n        \n        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n        # cv2.imshow('Real-time Analysis', processed_frame)\n        # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n        #     break\n    else:\n        break\n\n# Release the video capture and video write objects\ncap.release()\nout.release()\n\n# Close all the frames\n# cv2.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        Finally, lets convert the output <code>.avi</code> video to <code>.mp4</code> format for notebook playback and display it:\n    </p>\n</div>","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Convert the .avi video generated by  traffic density estimation app to .mp4 format for compatibility with notebook display\n!ffmpeg -y -loglevel panic -i /kaggle/working/traffic_density_analysis.avi traffic_density_analysis.mp4\n\n# Embed and display the processed sample video within the notebook\nVideo(\"traffic_density_analysis.mp4\", embed=True, width=960)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #eae8fa; font-size: 120%; text-align: center;\">\n    <strong>\n        🌐 For comprehensive insights, extensive code, and additional resources, visit the project's \n        <a href=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/tree/master\" style=\"color: #141140; text-decoration: none;\">\n            <em><u>GitHub Repository</u></em>\n        </a> \n        🌐\n    </strong>\n</div>\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}}]}